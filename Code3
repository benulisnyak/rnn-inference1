import torch
import numpy as np
from scipy.integrate import solve_ivp
import matplotlib.pyplot as plt
import snntorch as snn
from snntorch import spikeplot as splt
from snntorch import surrogate
import torch.nn as nn
import matplotlib.pyplot as plt

torch.manual_seed(0) 

def rossler(t, state, a=0.2, b=0.2, c=5.7):
  x, y, z = state
  dxdt = -y - z
  dydt = x + a*y
  dzdt = b + z*(x - c)
  return [dxdt, dydt, dzdt]

#Solve RÃ¶ssler system
t_span = (0, 500)
t_eval = np.linspace(*t_span, 10000)
sol = solve_ivp(rossler, t_span, [1, 1, 1], t_eval=t_eval)

#Normalize the data
data = torch.tensor(sol.y, dtype=torch.float32).T
data = (data - data.mean(0)) / data.std(0)

#Network stuff:
num_hidden = 200
num_outputs = 3
beta = 0.88 #decay parameter - larger values mean faster firing rate


#Define Network
class SelfEvolvingSNN(nn.Module):
  def __init__(self):
    super().__init__()
    self.fc1 = nn.Linear(num_outputs, num_hidden) #.fc1 applies linear transform to the input data
    #for param1 in self.fc1.parameters():
      #param1.requires_grad = False
    self.recurrent = nn.Linear(num_hidden, num_hidden)
    self.lif1 = snn.Leaky(beta = beta, spike_grad=surrogate.fast_sigmoid())
   
    self.readout = nn.Linear(num_hidden, 3)
    nn.init.xavier_uniform_(self.recurrent.weight)
  def forward(self, steps, init_state):
    mem1 = self.lif1.init_leaky()
    spk1 = torch.zeros_like(mem1)
    outputs = []
    spk_record = []
    out = init_state.unsqueeze(0)  #adds a dimension for formatting
    feedback = torch.zeros((1, self.fc1.out_features), device=init_state.device)
    bias = 0.15
    for _ in range(steps):
      h =  self.recurrent(feedback)*0.25 + bias + self.fc1(out)*0.25 #current supplied to the network
      spk1, mem1 = self.lif1(h, mem1)
      feedback = spk1 #works as connections in the network to supply other neurons with current when it spikes
      out = self.readout(spk1)
      outputs.append(out)
      spk_record.append(spk1)
    return torch.cat(outputs), torch.stack(spk_record).squeeze(1)


######## Training
model = SelfEvolvingSNN()

#Optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)
loss_fn = nn.MSELoss()

#Initial state 
init_state = data[0]  # [X0, Y0, Z0]
# Training loop
epochs = 10 #Number of backpropogations loops
time_steps = 1000 #shorter for training
num_inputs = 3


for epoch in range(epochs):
  optimizer.zero_grad()
  output, _ = model(time_steps, init_state)
  target = data[:time_steps]
  loss = loss_fn(output.squeeze(), target)
  loss.backward()
  optimizer.step()
  print(f"Epoch {epoch+1}, Loss: {loss.item():.4f}")

############ End of training 

with torch.no_grad(): #generates network output after training
  generated, test_spikes = model(time_steps, init_state)
  generated = generated.squeeze()


#Creates plots
plt.figure(figsize=(12, 4))
plt.plot(data[:time_steps, 0], label="True X")
plt.plot(generated[:, 0], label="Generated X", linestyle='--')
plt.legend()
plt.title("Comparison of X Dynamics")
plt.show()

plt.figure(figsize=(12, 4))
plt.plot(data[:time_steps, 1], label="True Y")
plt.plot(generated[:, 1], label="Generated Y", linestyle='--')
plt.legend()
plt.title("Comparison of Y Dynamics")
plt.show()

plt.figure(figsize=(12, 4))
plt.plot(data[:time_steps, 2], label="True Z")
plt.plot(generated[:, 2], label="Generated Z", linestyle='--')
plt.legend()
plt.title("Comparison of Z Dynamics")
plt.show()

fig, ax = plt.subplots(figsize=(12, 6))
splt.raster(test_spikes, ax, s=2, c="black")
plt.title("Raster Plot of Spikes")
plt.xlabel("Time step")
plt.ylabel("Neuron index")
plt.show()
